tidy_skipgrams = transcripts %>%
unnest_tokens(ngram, transcript, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
tidyr::unite(skipgramID, url, ngramID) %>%
unnest_tokens(word, ngram)
tidy_skipgrams
unigram_probs = transcripts %>%
unnest_tokens(word, transcript) %>%
count(word, sort = TRUE) %>%
mutate(p = n / sum(n))
unigram_probs
skipgram_probs = tidy_skipgrams %>%
pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
mutate(p = n / sum(n))
normalized_prob = skipgram_probs %>%
filter(n > 20) %>%
rename(word1 = item1, word2 = item2) %>%
left_join(unigram_probs %>%
select(word1 = word, p1 = p),
by = "word1") %>%
left_join(unigram_probs %>%
select(word2 = word, p2 = p),
by = "word2") %>%
mutate(p_together = p / p1 / p2)
normalized_prob
normalized_prob %>%
filter(word1 == "education") %>%
arrange(-p_together)
pmi_matrix = normalized_prob %>%
mutate(pmi = log10(p_together)) %>%
cast_sparse(word1, word2, pmi)
pmi_matrix
#remove missing data
pmi_matrix@x[is.na(pmi_matrix@x)] = 0
#run SVD
pmi_svd = irlba(pmi_matrix, 256, maxit = 500)
library(irlba)
#run SVD
pmi_svd = irlba(pmi_matrix, 256, maxit = 500)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE, comment = NA)
library(tidyverse)
library(tidyverse)
library(rvest)
library(rvest)
library(tidyverse)
ted = read_csv("data/ted_main.csv")
ted['url']
urls = ted %>% select(url)
urls
read_html(urs[1])
read_html(urls[1])
urls[1]
read_html(urls[1,])
urls[1,]
urls = ted %>% pull(url)
urls
read_html(urls[1])
urls[1]
urls = ted %>% pull(url) %>% str_trim()
urls
read_html(urls[1])
urls[1] %>% read_html()
urls[1]
urls[1] %>% read_html()
urls[1] %>% read_html() %>% html_nodes("div")
urls[1] %>% read_html() %>% html_nodes("div") %>% html_text()
urls[1] %>% read_html() %>% html_nodes("div span") %>% html_text()
pulled = urls[1] %>% read_html() %>% html_nodes("div span") %>% html_text()
pulled[4]
pulled[4] %>% str_trim()
pulled[4] %>% str_trim() %>% str_replace('views', "")
pulled[4] %>% str_trim() %>% str_replace('views', "")
pulled[4] %>% str_replace('views', "")
pulled[4] %>% str_replace('views', "") %>% str_trim()
pulled[4] %>% str_replace('views', "") %>% str_trim() %>% as.numeric()
pulled[4] %>% str_replace('views', "") %>% str_trim() %>% parse_number()
ted = read_csv("data/ted_main.csv")
urls = ted %>% pull(url) %>% str_trim()
views = c()
for (i in 1:10){ # length(urls)
pulled = urls[i] %>% read_html() %>% html_nodes("div span") %>% html_text()
out = pulled[4] %>% str_replace('views', "") %>% str_trim() %>% parse_number()
views = c(views, out)
}
views
ted = read_csv("data/ted_main.csv")
urls = ted %>% pull(url) %>% str_trim()
views = c()
for (i in 1:length(urls)){
pulled = urls[i] %>% read_html() %>% html_nodes("div span") %>% html_text()
out = pulled[4] %>% str_replace('views', "") %>% str_trim() %>% parse_number()
views = c(views, out)
}
views
tibble('urls' = urls, 'views' = views)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE, comment = NA)
library(tidyverse)
library(rvest)
ted = read_csv("data/ted_main.csv")
ted = read_csv("data/ted_main.csv")
urls = ted %>% pull(url) %>% str_trim()
urls
final = tibble('url' = urls, 'views' = NA)
final
parser_function = function(df){
#### Function to parse our dats ####
for (i in seq_along(urls)){
url = urls[i]
page = tryCatch(read_html(url), error=function(err) NA)
if(is.na(page) == FALSE){
views = page %>%
html_nodes("div span") %>%
html_text()
views = views[4] %>%
str_replace('views', "") %>%
str_trim() %>%
parse_number()
df[["views"]][i] = views
}
else{
df[["views"]][i] = NA
}
}
return(df)
}
ted = read_csv("data/ted_main.csv")
urls = ted %>% pull(url) %>% str_trim()
final = tibble('url' = urls, 'views' = NA)
parser_function = function(df){
#### Function to parse our dats ####
for (i in seq_along(urls)){
url = urls[i]
page = tryCatch(read_html(url), error=function(err) NA)
if(is.na(page) == FALSE){
views = page %>%
html_nodes("div span") %>%
html_text()
views = views[4] %>%
str_replace('views', "") %>%
str_trim() %>%
parse_number()
df[["views"]][i] = views
}
else{
df[["views"]][i] = NA
}
}
return(df)
}
ted_views = parser_function(final)
final
parser_function = function(df){
#### Function to parse our dats ####
for (i in seq_along(urls)){
url = urls[i]
page = tryCatch(read_html(url), error=function(err) NA)
if(is.na(page) == FALSE){
views = page %>%
html_nodes("div span") %>%
html_text()
views = views[4] %>%
str_replace('views', "") %>%
str_trim()
df[["views"]][i] = views
}
else{
df[["views"]][i] = NA
}
}
return(df)
}
ted_views = parser_function(final)
ted_views
write.csv(ted_views, "ted_views.csv", row.names = FALSE)
views = read_csv('ted_views.csv')
View(views)
is.na(views)
is.na(views).sum()
is.na(views) %>% sum()
views[['views']]
views[['views']] %>% parse_number()
views[['views']] %>% parse_number()
views[['views']]
views[['views']][970]
views[['views']][970] = NA
views[['views']] %>% parse_number()
views[['views']] = views[['views']] %>% parse_number()
views
write.csv(views, "ted_views.csv", row.names = FALSE)
views = read_csv('ted_views.csv')
views
views
views %>% which(is.na(views) == TRUE)
ted = read_csv("ted_views.csv")
views %>%
filter(is.na(views) == TRUE)
ted_views
ted = read_csv("ted_views.csv")
ted %>%
filter(is.na(views) == TRUE)
miss = ted %>% filter(is.na(views) == TRUE)
miss
url = 'https://www.ted.com/talks/eddi_reader_sings_kiteflyer_s_hill'
views = page %>%
html_nodes("div span") %>%
html_text()
page %>%
html_nodes("div span")
url = 'https://www.ted.com/talks/eddi_reader_sings_kiteflyer_s_hill'
page = read_html(url)
views = page %>%
html_nodes("div span") %>%
html_text()
views
views = views[5] %>%
str_replace('views', "") %>%
str_trim()
views
miss
#### Function to scrape views ####
parser_function = function(df){
for (i in seq_along(urls)){
url = urls[i]
page = tryCatch(read_html(url), error=function(err) NA)
if(is.na(page) == FALSE){
views = page %>%
html_nodes("div span") %>%
html_text()
views = views[5] %>%
str_replace('views', "") %>%
str_trim()
df[["views"]][i] = views
}
else{
df[["views"]][i] = NA
}
}
return(df)
}
miss = parser_function(miss)
miss
ted = read_csv("ted_views.csv")
miss = ted %>% filter(is.na(views) == TRUE)
#### Function to scrape views ####
parser_function = function(df){
for (i in seq_along(urls)){
url = urls[i]
page = tryCatch(read_html(url), error=function(err) NA)
if(is.na(page) == FALSE){
views = page %>%
html_nodes("div span") %>%
html_text()
views = views[5] %>%
str_replace('views', "") %>%
str_trim()
df[["views"]][i] = views
}
else{
df[["views"]][i] = NA
}
}
return(df)
}
miss_views = parser_function(miss)
count = 0
#### Function to scrape views ####
parser_function = function(df){
for (i in seq_along(urls)){
count = count + 1
url = urls[i]
page = tryCatch(read_html(url), error=function(err) NA)
if(is.na(page) == FALSE){
views = page %>%
html_nodes("div span") %>%
html_text()
views = views[5] %>%
str_replace('views', "") %>%
str_trim()
df[["views"]][i] = views
}
else{
df[["views"]][i] = NA
}
}
return(df)
}
miss_views = parser_function(miss)
miss
#### Function to scrape views ####
parser_function = function(df){
urls = miss[['url']]
for (i in seq_along(urls)){
url = urls[i]
page = tryCatch(read_html(url), error=function(err) NA)
if(is.na(page) == FALSE){
views = page %>%
html_nodes("div span") %>%
html_text()
views = views[5] %>%
str_replace('views', "") %>%
str_trim()
df[["views"]][i] = views
}
else{
df[["views"]][i] = NA
}
}
return(df)
}
miss_views = parser_function(miss)
miss_views
miss_views
miss_views %>% filter(is.na(views) == TRUE)
miss_views[['views']] = miss_views[['views']] %>% parse_number()
miss_views
ted = ted %>% left_join(miss_views)
ted
ted %>% filter(is.na(views) == TRUE)
ted %>% filter(is.na(ted) == TRUE)
ted %>% filter(is.na(views) == TRUE)
miss_views
ted
ted = ted %>% left_join(miss_views, by = 'url')
ted
ted %>% filter(is.na(views) == TRUE)
ted
ted = read_csv("ted_views.csv")
miss_views
left_join(ted, miss_views, by = 'url')
merge(ted, miss_views, by = 'url')
left_join(ted, miss_views, by = 'url')
ted_inter = left_join(ted, miss_views, by = 'url')
ted_inter %>% filter(is.na(views) == TRUE)
ted_inter
ted_inter[['views.x']]
ted_inter[['views.x']][which(is.na(ted_inter[['views.x']]) == TRUE)]
ted_inter[['views.x']][which(is.na(ted_inter[['views.x']]) == TRUE)] = 0
ted_inter[['views.y']][which(is.na(ted_inter[['views.y']]) == TRUE)] = 0
ted_inter[['views']] = ted_inter[['views.x']] + ted_inter[['views.y']]
ted_inter
ted_inter %>% select(url, views)
ted_inter = ted_inter %>% select(url, views)
ted_inter[['views']]
ted_inter[['views']][970]
ted_inter %>% View()
ted_inter[['views']]
ted = read_csv("ted_views.csv")
ted[18]
ted
ted[18,]
ted_inter[['views']]
ted_inter[18,]
ted_inter[['views']]
miss_views
ted[['views']]
miss_views[['views']]
miss_views[['views']][18]
miss_views[['views']]
miss_views
miss_views[['views']][18]
miss_views[['views']][19]
miss_views[['views']][17]
miss_views[['views']]
View(miss_views)
miss_views[['views']]
miss_views[['views']] == 'Karin Ã–berg'
miss_views = parser_function(miss)
miss_views
miss_views[['views']]
miss_views[['views']] %>% parse_number()
miss_views[['views']][18]
miss_views[['views']][18] = NA
miss_views[['views']]
miss_views[['views']] = miss_views[['views']] %>% parse_number()
ted_inter = left_join(ted, miss_views, by = 'url')
ted_inter
miss_views[['views']]
ted_inter = left_join(ted, miss_views, by = 'url')
ted_inter[['views.x']][which(is.na(ted_inter[['views.x']]) == TRUE)] = 0
ted_inter[['views.y']][which(is.na(ted_inter[['views.y']]) == TRUE)] = 0
ted_inter[['views']] = ted_inter[['views.x']] + ted_inter[['views.y']]
ted_inter = ted_inter %>% select(url, views)
ted_inter
ted_inter %>% filter(is.na(views) == TRUE)
ted_inter %>% filter(views == 0)
ted_inter[['views']][which(ted_inter[['views']] == 0)]
ted_inter[['views']][which(ted_inter[['views']] == 0)] = NA
ted_inter
write.csv(ted_inter, 'ted_views.csv', row.names = FALSE)
ted = read_csv("ted_views.csv")
ted
ted = read_csv("ted_views.csv")
View(ted)
ted %>%
filter(is.na(views) == TRUE)
page = read_html('https://www.ted.com/talks/david_byrne_sings_nothing_but_flowers')
views = page %>%
html_nodes("div span") %>%
html_text()
views
views = views[6] %>%
str_replace('views', "") %>%
str_trim()
views
#### Function to scrape views ####
parser_function = function(df){
urls = df[['url']]
for (i in seq_along(urls)){
url = urls[i]
page = tryCatch(read_html(url), error=function(err) NA)
if(is.na(page) == FALSE){
views = page %>%
html_nodes("div span") %>%
html_text()
views = views[5] %>%
str_replace('views', "") %>%
str_trim()
df[["views"]][i] = views
}
else{
df[["views"]][i] = NA
}
}
return(df)
}
miss = ted %>%
filter(is.na(views) == TRUE)
miss
miss_views = parser_function(miss)
parser_function = function(df){
urls = df[['url']]
for (i in seq_along(urls)){
url = urls[i]
page = tryCatch(read_html(url), error=function(err) NA)
if(is.na(page) == FALSE){
views = page %>%
html_nodes("div span") %>%
html_text()
views = views[6] %>%
str_replace('views', "") %>%
str_trim()
df[["views"]][i] = views
}
else{
df[["views"]][i] = NA
}
}
return(df)
}
#### Function to scrape views ####
parser_function = function(df){
urls = df[['url']]
for (i in seq_along(urls)){
url = urls[i]
page = tryCatch(read_html(url), error=function(err) NA)
if(is.na(page) == FALSE){
views = page %>%
html_nodes("div span") %>%
html_text()
views = views[6] %>%
str_replace('views', "") %>%
str_trim()
df[["views"]][i] = views
}
else{
df[["views"]][i] = NA
}
}
return(df)
}
miss_views = parser_function(miss)
miss_views
miss_views[['views']]
miss_views[['views']] %>% parse_number()
miss_views[['views']][2] = NA
miss_views[['views']] %>% parse_number()
remaining_views = c(755883, NA, 1033834, NA, NA, 1404472, NA, 10594943, NA, 773327, NA, NA, NA)
miss
remaining_views = c(755883, NA, 1033834, 762089, NA, 1404472, NA, 10594943, NA, 773327, 932520, 1047416, 307990)
miss
miss[['views']] = remaining_views
miss
left_join(ted, miss_views, by = 'url')
left_join(ted, miss, by = 'url')
ted_inter = left_join(ted, miss, by = 'url')
ted_inter[['views.x']][which(is.na(ted_inter[['views.x']]) == TRUE)] = 0
ted_inter[['views.y']][which(is.na(ted_inter[['views.y']]) == TRUE)] = 0
ted_inter[['views']] = ted_inter[['views.x']] + ted_inter[['views.y']]
ted_inter = ted_inter %>% select(url, views)
ted_inter
ted_inter[['views']][which(ted_inter[['views']] == 0)] = NA
ted_inter %>%
filter(is.na(views) == TRUE)
ted_inter
write.csv(ted_inter, "ted_views.csv", row.names = FALSE)
ted_inter
write.csv(ted_inter, "ted_views.csv", row.names = FALSE)
ted = read_csv("ted_views.csv")
ted
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE, comment = NA)
library(tidyverse)
library(rvest)
ted = read_csv("data/ted_full.csv")
ted = read_csv("data/ted_full.csv")
ted_views = read_csv("data/ted_views.csv")
ted_views
ted
ted_views
names(ted_views) = c('url', 'view_2020')
ted = left_join(ted, ted_views, by = 'url')
View(ted)
write.csv(ted, 'ted_full_v2.csv', row.names = FALSE)
